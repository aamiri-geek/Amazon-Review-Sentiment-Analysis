{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec3f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import bz2\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "738bec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitReviewsLabels(lines):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for review in tqdm(lines):\n",
    "        rev = reviewToX(review)\n",
    "        label = reviewToY(review)\n",
    "        reviews.append(rev[:512])\n",
    "        labels.append(label)\n",
    "    return reviews, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3bc4a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviewToY(review):\n",
    "    return 0 if review.split(' ')[0] == '__label__1' else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3a8682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviewToX(review):\n",
    "    review = review.split(' ', 1)[1][:-1].lower()\n",
    "    review = re.sub('\\d','0',review)\n",
    "    if 'www.' in review or 'http:' in review or 'https:' in review or '.com' in review:\n",
    "        review = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", review)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f6564db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = bz2.BZ2File('train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d18a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = train_file.readlines()\n",
    "test_lines = test_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf199ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = [x.decode('utf-8') for x in train_lines[:20000]]\n",
    "test_lines = [x.decode('utf-8') for x in test_lines[:2000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d82049f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 85975.28it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 74185.57it/s]\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = splitReviewsLabels(train_lines)\n",
    "test_x,test_y = splitReviewsLabels(test_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b8687cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "train_x[8]\n",
    "print(train_y[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51af3bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "test_y = encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b217535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8faa057d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.48715, 0.51285])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi=np.array([sum(train_y==0)/len(train_y),sum(train_y==1)/len(train_y)])\n",
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b54bdc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(train_x)\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x).todense()\n",
    "xtest_count =  count_vect.transform(test_x).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "665a9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordFreq = pd.DataFrame(columns=['words','class1','class2'])\n",
    "wordFreq['words'] = count_vect.get_feature_names()\n",
    "\n",
    "x_train_class1 = xtrain_count[train_y==0]\n",
    "x_train_class2 = xtrain_count[train_y==1]\n",
    "\n",
    "count_class1 = np.sum(x_train_class1,axis=0)\n",
    "count_class2 = np.sum(x_train_class2,axis=0)\n",
    "\n",
    "vocab_size1 = len(np.where(count_class1==0)[1])\n",
    "vocab_size2 = len(np.where(count_class2==0)[1])\n",
    "\n",
    "alpha=10\n",
    "count_class1 = np.array( (count_class1+alpha) /(np.sum(count_class1)+vocab_size1 +1))\n",
    "count_class2 = np.array( (count_class2+alpha) /(np.sum(count_class2)+vocab_size2 +1))\n",
    "\n",
    "wordFreq['class1'] = pd.Series(count_class1.ravel())\n",
    "wordFreq['class2'] = pd.Series(count_class2.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d53bb76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is: 0.8814\n"
     ]
    }
   ],
   "source": [
    "train_preds = np.zeros(len(xtrain_count))\n",
    "for i in range(len(xtrain_count)):\n",
    "    idx = np.where(xtrain_count[i,:]!=0)[1]\n",
    "    lh1 = wordFreq['class1'].iloc[idx].prod()\n",
    "    lh2 = wordFreq['class2'].iloc[idx].prod()\n",
    "    posterior1 = lh1*pi[0]\n",
    "    posterior2 = lh2 * pi[1]\n",
    "\n",
    "    if posterior1>posterior2:\n",
    "        train_preds[i] = 0\n",
    "    else:\n",
    "        train_preds[i] = 1\n",
    "\n",
    "\n",
    "matches = np.sum(train_y==train_preds)\n",
    "print('Train accuracy is: '+str(matches/len(train_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "292d9665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.8385\n"
     ]
    }
   ],
   "source": [
    "test_preds = np.zeros(len(xtest_count))\n",
    "for i in range(len(xtest_count)):\n",
    "    idx = np.where(xtest_count[i,:]!=0)[1]\n",
    "    lh1 = wordFreq['class1'].iloc[idx].prod()\n",
    "    lh2 = wordFreq['class2'].iloc[idx].prod()\n",
    "    posterior1 = lh1*pi[0]\n",
    "    posterior2 = lh2 * pi[1]\n",
    "\n",
    "    if posterior1>posterior2:\n",
    "        test_preds[i] = 0\n",
    "    else:\n",
    "        test_preds[i] = 1\n",
    "\n",
    "    temp = 1\n",
    "\n",
    "matches = np.sum(test_y==test_preds)\n",
    "print('Validation accuracy is: '+str(matches/len(test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a19cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
